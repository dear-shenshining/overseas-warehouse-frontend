# 爬虫分批处理机制说明

## 📋 问题背景

Vercel 无服务器函数有 **300 秒超时限制**，如果一次性处理太多追踪号会导致超时错误。

## ✅ 解决方案：分批处理机制

### 核心思路

1. **每次处理 50 个追踪号**（`BATCH_SIZE = 50`）
2. **失败的追踪号自动加入队列后面**（通过更新 `updated_at` 时间戳）
3. **继续处理下一批 50 个**
4. **最多处理 10 个批次**（避免无限循环，10 × 50 = 500 个）

### 处理流程

```
开始
  ↓
获取第一批 50 个追踪号（按 updated_at 排序）
  ↓
处理这批追踪号
  ├─ 成功 → 统计成功数
  ├─ 失败 → 更新 updated_at，加入队列后面
  └─ 跳过 → 统计跳过数
  ↓
批次完成，统计结果
  ↓
还有待处理的追踪号？
  ├─ 是 → 继续下一批（最多 10 批）
  └─ 否 → 结束
  ↓
返回统计结果
```

## 🔧 实现细节

### 1. 批次大小

```typescript
const BATCH_SIZE = 50 // 每批处理 50 个追踪号
```

### 2. 最大批次限制

```typescript
const MAX_BATCHES = 10 // 最多处理 10 个批次
```

**原因**：
- 避免无限循环（如果所有追踪号都失败，会一直重试）
- 控制单次任务总时长（10 批 × 50 个 × 平均 3 秒/个 ≈ 25 分钟，远小于 300 秒）

### 3. 失败处理机制

当追踪号处理失败时：

```typescript
// 更新 updated_at，让失败的记录排到队列后面
await bumpSearchUpdatedAt(trackingNumber)
```

**效果**：
- 失败的追踪号会排在队列后面
- 下一批会优先处理新的或更早的追踪号
- 失败的追踪号会在后续批次中重试

### 4. 去重机制

使用 `Set` 记录已处理的追踪号，避免同一批次内重复处理：

```typescript
const processedSet = new Set<string>()
```

### 5. 批次间延迟

每个批次之间延迟 2 秒，避免数据库压力过大：

```typescript
await new Promise((resolve) => setTimeout(resolve, 2000))
```

## 📊 统计信息

返回的统计信息包括：

- **total**：总处理数量
- **success**：成功数量
- **failed**：失败数量
- **skipped**：跳过数量（已完成状态）
- **retries**：总重试次数
- **batches**：处理的批次数（新增）

## 🎯 优势

### 1. 避免超时

- 每批处理时间可控（50 个 × 平均 3 秒 ≈ 2.5 分钟）
- 即使处理 10 批，总时间也远小于 300 秒限制

### 2. 失败自动重试

- 失败的追踪号自动加入队列
- 下一批会继续尝试处理
- 不需要手动干预

### 3. 资源利用优化

- 批次间有延迟，避免数据库压力过大
- 优先处理新的追踪号，失败的排在后面

### 4. 可扩展性

- 可以调整 `BATCH_SIZE` 和 `MAX_BATCHES`
- 根据实际情况优化批次大小

## 📝 日志输出示例

```
📋 开始分批处理追踪号（每批 50 个，最多 10 批）...
============================================================

🔄 开始处理第 1 批（最多 10 批）...
------------------------------------------------------------

正在处理追踪号：628327933074
--------------------------------------------------
✅ 成功处理追踪号：628327933074 (重试 0 次)

正在处理追踪号：628327933075
--------------------------------------------------
⚠️ 追踪号 628327933075 处理失败，准备重试 (1/5)...
✅ 成功处理追踪号：628327933075 (重试 1 次)

...

📊 第 1 批完成：处理 50 个，成功 48，失败 2，跳过 0

✅ 本批次处理完成，继续下一批...

🔄 开始处理第 2 批（最多 10 批）...
------------------------------------------------------------
...
```

## ⚙️ 配置参数

可以在 `lib/logistics-crawler.ts` 中调整以下参数：

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `BATCH_SIZE` | 50 | 每批处理的追踪号数量 |
| `MAX_BATCHES` | 10 | 最多处理的批次数 |
| `MAX_RETRIES` | 5 | 每个追踪号的最大重试次数 |
| `MAX_RETRY_DELAY_MS` | 3000 | 单次重试的最大延迟（毫秒） |

## 🔍 工作流程示例

### 场景 1：正常处理

```
第 1 批：50 个 → 成功 48，失败 2
第 2 批：50 个（包含第 1 批失败的 2 个）→ 成功 50
第 3 批：30 个 → 成功 30
结束（没有更多待处理）
```

### 场景 2：大量失败

```
第 1 批：50 个 → 成功 20，失败 30
第 2 批：50 个（包含第 1 批失败的 30 个）→ 成功 25，失败 25
第 3 批：50 个（包含之前失败的）→ 成功 30，失败 20
...
第 10 批：50 个 → 成功 40，失败 10
结束（达到最大批次限制）
```

## 💡 最佳实践

1. **监控批次数量**：如果经常达到 10 批，考虑：
   - 增加 `MAX_BATCHES`
   - 检查网络连接是否稳定
   - 检查追踪号是否有效

2. **调整批次大小**：
   - 如果网络快，可以增加 `BATCH_SIZE`
   - 如果经常超时，可以减少 `BATCH_SIZE`

3. **定期运行**：
   - 建议定期运行爬虫（如每小时一次）
   - 每次处理一部分，避免积压

---

**更新日期**：2025-01-XX  
**相关文件**：
- `lib/logistics-crawler.ts`
- `CRAWLER_RETRY_MECHANISM.md`

